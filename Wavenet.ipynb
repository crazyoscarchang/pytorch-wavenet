{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QA7w-IgJEcoF"
   },
   "source": [
    "# **Wavenet Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6mmn8-YIj9Cu"
   },
   "outputs": [],
   "source": [
    "# http://pytorch.org/\n",
    "from os import path\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LYFCYxqVGUL_"
   },
   "source": [
    "**Wavenet Main Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "beDc2kuNVRbM"
   },
   "source": [
    "**data loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "xj1Z_WlcSV1F"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Show raw audio and mu-law encode samples to make input source\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "def load_audio(filename, sample_rate=16000, trim=True, trim_frame_length=2048):\n",
    "    audio, _ = librosa.load(filename, sr=sample_rate, mono=True)\n",
    "    audio = audio.reshape(-1, 1)\n",
    "\n",
    "    if trim > 0:\n",
    "        audio, _ = librosa.effects.trim(audio, frame_length=trim_frame_length)\n",
    "\n",
    "    return audio\n",
    "\n",
    "\n",
    "def one_hot_encode(data, channels=256):\n",
    "    one_hot = np.zeros((data.size, channels), dtype=float)\n",
    "    one_hot[np.arange(data.size), data.ravel()] = 1\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def one_hot_decode(data, axis=1):\n",
    "    decoded = np.argmax(data, axis=axis)\n",
    "\n",
    "    return decoded\n",
    "\n",
    "\n",
    "def mu_law_encode(audio, quantization_channels=256):\n",
    "    \"\"\"\n",
    "    Quantize waveform amplitudes.\n",
    "    Reference: https://github.com/vincentherrmann/pytorch-wavenet/blob/master/audio_data.py\n",
    "    \"\"\"\n",
    "    mu = float(quantization_channels - 1)\n",
    "    quantize_space = np.linspace(-1, 1, quantization_channels)\n",
    "\n",
    "    quantized = np.sign(audio) * np.log(1 + mu * np.abs(audio)) / np.log(mu + 1)\n",
    "    quantized = np.digitize(quantized, quantize_space) - 1\n",
    "\n",
    "    return quantized\n",
    "\n",
    "\n",
    "def mu_law_decode(output, quantization_channels=256):\n",
    "    \"\"\"\n",
    "    Recovers waveform from quantized values.\n",
    "    Reference: https://github.com/vincentherrmann/pytorch-wavenet/blob/master/audio_data.py\n",
    "    \"\"\"\n",
    "    mu = float(quantization_channels - 1)\n",
    "\n",
    "    expanded = (output / quantization_channels) * 2. - 1\n",
    "    waveform = np.sign(expanded) * (\n",
    "                   np.exp(np.abs(expanded) * np.log(mu + 1)) - 1\n",
    "               ) / mu\n",
    "\n",
    "    return waveform\n",
    "\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, data_dir, sample_rate=16000, in_channels=256, trim=True):\n",
    "        super(Dataset, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.sample_rate = sample_rate\n",
    "        self.trim = trim\n",
    "\n",
    "        self.root_path = data_dir\n",
    "        self.filenames = [x for x in sorted(os.listdir(data_dir))]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filepath = os.path.join(self.root_path, self.filenames[index])\n",
    "\n",
    "        raw_audio = load_audio(filepath, self.sample_rate, self.trim)\n",
    "\n",
    "        encoded_audio = mu_law_encode(raw_audio, self.in_channels)\n",
    "        encoded_audio = one_hot_encode(encoded_audio, self.in_channels)\n",
    "\n",
    "        return encoded_audio\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "\n",
    "class DataLoader(data.DataLoader):\n",
    "    def __init__(self, data_dir, receptive_fields,\n",
    "                 sample_size=0, sample_rate=16000, in_channels=256,\n",
    "                 batch_size=1, shuffle=True):\n",
    "        \"\"\"\n",
    "        DataLoader for WaveNet\n",
    "        :param data_dir:\n",
    "        :param receptive_fields: integer. size(length) of receptive fields\n",
    "        :param sample_size: integer. number of timesteps to train at once.\n",
    "                            sample size has to be bigger than receptive fields.\n",
    "                            |-- receptive field --|---------------------|\n",
    "                            |------- samples -------------------|\n",
    "                            |---------------------|-- outputs --|\n",
    "        :param sample_rate: sound sampling rates\n",
    "        :param in_channels: number of input channels\n",
    "        :param batch_size:\n",
    "        :param shuffle:\n",
    "        \"\"\"\n",
    "        dataset = Dataset(data_dir, sample_rate, in_channels)\n",
    "\n",
    "        super(DataLoader, self).__init__(dataset, batch_size, shuffle)\n",
    "\n",
    "        if sample_size <= receptive_fields:\n",
    "            raise Exception(\"sample_size has to be bigger than receptive_fields\")\n",
    "\n",
    "        self.sample_size = sample_size\n",
    "        self.receptive_fields = receptive_fields\n",
    "\n",
    "        self.collate_fn = self._collate_fn\n",
    "\n",
    "    def calc_sample_size(self, audio):\n",
    "        return self.sample_size if len(audio[0]) >= self.sample_size\\\n",
    "                                else len(audio[0])\n",
    "\n",
    "    @staticmethod\n",
    "    def _variable(data):\n",
    "        tensor = torch.from_numpy(data).float()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.autograd.Variable(tensor.cuda())\n",
    "        else:\n",
    "            return torch.autograd.Variable(tensor)\n",
    "\n",
    "    def _collate_fn(self, audio):\n",
    "        audio = np.pad(audio, [[0, 0], [self.receptive_fields, 0], [0, 0]], 'constant')\n",
    "\n",
    "        if self.sample_size:\n",
    "            sample_size = self.calc_sample_size(audio)\n",
    "\n",
    "            while sample_size > self.receptive_fields:\n",
    "                inputs = audio[:, :sample_size, :]\n",
    "                targets = audio[:, self.receptive_fields:sample_size, :]\n",
    "\n",
    "                yield self._variable(inputs),\\\n",
    "                      self._variable(one_hot_decode(targets, 2))\n",
    "\n",
    "                audio = audio[:, sample_size-self.receptive_fields:, :]\n",
    "                sample_size = self.calc_sample_size(audio)\n",
    "        else:\n",
    "            targets = audio[:, self.receptive_fields:, :]\n",
    "            yield self._variable(audio),\\\n",
    "                   self._variable(one_hot_decode(targets, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "A6MzPBXeRc-f"
   },
   "outputs": [],
   "source": [
    "class InputSizeError(Exception):\n",
    "    def __init__(self, input_size, receptive_fields, output_size):\n",
    "\n",
    "        message = 'Input size has to be larger than receptive_fields\\n'\n",
    "        message += 'Input size: {0}, Receptive fields size: {1}, Output size: {2}'.format(\n",
    "            input_size, receptive_fields, output_size)\n",
    "\n",
    "        super(InputSizeError, self).__init__(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k94_LHmTVWx5"
   },
   "source": [
    "**Actual Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "wqRQ8HlyGTUw"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Neural network modules for WaveNet\n",
    "References :\n",
    "    https://arxiv.org/pdf/1609.03499.pdf\n",
    "    https://github.com/ibab/tensorflow-wavenet\n",
    "    https://qiita.com/MasaEguchi/items/cd5f7e9735a120f27e2a\n",
    "    https://github.com/musyoku/wavenet/issues/4\n",
    "\"\"\"\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DilatedCausalConv1d(torch.nn.Module):\n",
    "    \"\"\"Dilated Causal Convolution for WaveNet\"\"\"\n",
    "    def __init__(self, channels, dilation=1):\n",
    "        super(DilatedCausalConv1d, self).__init__()\n",
    "\n",
    "        self.conv = torch.nn.Conv1d(channels, channels,\n",
    "                                    kernel_size=2, stride=1,  # Fixed for WaveNet\n",
    "                                    dilation=dilation,\n",
    "                                    padding=0,  # Fixed for WaveNet dilation\n",
    "                                    bias=False)  # Fixed for WaveNet but not sure\n",
    "\n",
    "    def init_weights_for_test(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv1d):\n",
    "                m.weight.data.fill_(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class CausalConv1d(torch.nn.Module):\n",
    "    \"\"\"Causal Convolution for WaveNet\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(CausalConv1d, self).__init__()\n",
    "\n",
    "        # padding=1 for same size(length) between input and output for causal convolution\n",
    "        self.conv = torch.nn.Conv1d(in_channels, out_channels,\n",
    "                                    kernel_size=2, stride=1, padding=1,\n",
    "                                    bias=False)  # Fixed for WaveNet but not sure\n",
    "\n",
    "    def init_weights_for_test(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv1d):\n",
    "                m.weight.data.fill_(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv(x)\n",
    "\n",
    "        # remove last value for causal convolution\n",
    "        return output[:, :, :-1]\n",
    "\n",
    "\n",
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, res_channels, skip_channels, dilation):\n",
    "        \"\"\"\n",
    "        Residual block\n",
    "        :param res_channels: number of residual channel for input, output\n",
    "        :param skip_channels: number of skip channel for output\n",
    "        :param dilation:\n",
    "        \"\"\"\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.dilated = DilatedCausalConv1d(res_channels, dilation=dilation)\n",
    "        self.conv_res = torch.nn.Conv1d(res_channels, res_channels, 1)\n",
    "        self.conv_skip = torch.nn.Conv1d(res_channels, skip_channels, 1)\n",
    "\n",
    "        self.gate_tanh = torch.nn.Tanh()\n",
    "        self.gate_sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, skip_size):\n",
    "        \"\"\"\n",
    "        :param x:\n",
    "        :param skip_size: The last output size for loss and prediction\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        output = self.dilated(x)\n",
    "\n",
    "        # PixelCNN gate\n",
    "        gated_tanh = self.gate_tanh(output)\n",
    "        gated_sigmoid = self.gate_sigmoid(output)\n",
    "        gated = gated_tanh * gated_sigmoid\n",
    "\n",
    "        # Residual network\n",
    "        output = self.conv_res(gated)\n",
    "        input_cut = x[:, :, -output.size(2):]\n",
    "        output += input_cut\n",
    "\n",
    "        # Skip connection\n",
    "        skip = self.conv_skip(gated)\n",
    "        skip = skip[:, :, -skip_size:]\n",
    "\n",
    "        return output, skip\n",
    "\n",
    "\n",
    "class ResidualStack(torch.nn.Module):\n",
    "    def __init__(self, layer_size, stack_size, res_channels, skip_channels):\n",
    "        \"\"\"\n",
    "        Stack residual blocks by layer and stack size\n",
    "        :param layer_size: integer, 10 = layer[dilation=1, dilation=2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "        :param stack_size: integer, 5 = stack[layer1, layer2, layer3, layer4, layer5]\n",
    "        :param res_channels: number of residual channel for input, output\n",
    "        :param skip_channels: number of skip channel for output\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(ResidualStack, self).__init__()\n",
    "\n",
    "        self.layer_size = layer_size\n",
    "        self.stack_size = stack_size\n",
    "\n",
    "        self.res_blocks = self.stack_res_block(res_channels, skip_channels)\n",
    "\n",
    "    @staticmethod\n",
    "    def _residual_block(res_channels, skip_channels, dilation):\n",
    "        block = ResidualBlock(res_channels, skip_channels, dilation)\n",
    "\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            block = torch.nn.DataParallel(block)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            block.cuda()\n",
    "\n",
    "        return block\n",
    "\n",
    "    def build_dilations(self):\n",
    "        dilations = []\n",
    "\n",
    "        # 5 = stack[layer1, layer2, layer3, layer4, layer5]\n",
    "        for s in range(0, self.stack_size):\n",
    "            # 10 = layer[dilation=1, dilation=2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "            for l in range(0, self.layer_size):\n",
    "                dilations.append(2 ** l)\n",
    "\n",
    "        return dilations\n",
    "\n",
    "    def stack_res_block(self, res_channels, skip_channels):\n",
    "        \"\"\"\n",
    "        Prepare dilated convolution blocks by layer and stack size\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        res_blocks = []\n",
    "        dilations = self.build_dilations()\n",
    "\n",
    "        for dilation in dilations:\n",
    "            block = self._residual_block(res_channels, skip_channels, dilation)\n",
    "            res_blocks.append(block)\n",
    "\n",
    "        return res_blocks\n",
    "\n",
    "    def forward(self, x, skip_size):\n",
    "        \"\"\"\n",
    "        :param x:\n",
    "        :param skip_size: The last output size for loss and prediction\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        output = x\n",
    "        skip_connections = []\n",
    "\n",
    "        for res_block in self.res_blocks:\n",
    "            # output is the next input\n",
    "            output, skip = res_block(output, skip_size)\n",
    "            skip_connections.append(skip)\n",
    "\n",
    "        return torch.stack(skip_connections)\n",
    "\n",
    "\n",
    "class DensNet(torch.nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        \"\"\"\n",
    "        The last network of WaveNet\n",
    "        :param channels: number of channels for input and output\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(DensNet, self).__init__()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv1d(channels, channels, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(channels, channels, 1)\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.relu(x)\n",
    "        output = self.conv1(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.conv2(output)\n",
    "\n",
    "        output = self.softmax(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class WaveNetModule(torch.nn.Module):\n",
    "    def __init__(self, layer_size, stack_size, in_channels, res_channels):\n",
    "        \"\"\"\n",
    "        Stack residual blocks by layer and stack size\n",
    "        :param layer_size: integer, 10 = layer[dilation=1, dilation=2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "        :param stack_size: integer, 5 = stack[layer1, layer2, layer3, layer4, layer5]\n",
    "        :param in_channels: number of channels for input data. skip channel is same as input channel\n",
    "        :param res_channels: number of residual channel for input, output\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(WaveNetModule, self).__init__()\n",
    "\n",
    "        self.receptive_fields = self.calc_receptive_fields(layer_size, stack_size)\n",
    "\n",
    "        self.causal = CausalConv1d(in_channels, res_channels)\n",
    "\n",
    "        self.res_stack = ResidualStack(layer_size, stack_size, res_channels, in_channels)\n",
    "\n",
    "        self.densnet = DensNet(in_channels)\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_receptive_fields(layer_size, stack_size):\n",
    "        layers = [2 ** i for i in range(0, layer_size)] * stack_size\n",
    "        num_receptive_fields = np.sum(layers)\n",
    "\n",
    "        return int(num_receptive_fields)\n",
    "\n",
    "    def calc_output_size(self, x):\n",
    "        output_size = int(x.size(2)) - self.receptive_fields\n",
    "\n",
    "        self.check_input_size(x, output_size)\n",
    "\n",
    "        return output_size\n",
    "\n",
    "    def check_input_size(self, x, output_size):\n",
    "        if output_size < 1:\n",
    "            raise InputSizeError(int(x.size(2)), self.receptive_fields, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The size of timestep(3rd dimention) has to be bigger than receptive fields\n",
    "        :param x: Tensor[batch, timestep, channels]\n",
    "        :return: Tensor[batch, timestep, channels]\n",
    "        \"\"\"\n",
    "        output = x.transpose(1, 2)\n",
    "\n",
    "        output_size = self.calc_output_size(output)\n",
    "\n",
    "        output = self.causal(output)\n",
    "\n",
    "        skip_connections = self.res_stack(output, output_size)\n",
    "\n",
    "        output = torch.sum(skip_connections, dim=0)\n",
    "\n",
    "        output = self.densnet(output)\n",
    "\n",
    "        return output.transpose(1, 2).contiguous()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "La0wCcn9RiDO"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main model of WaveNet\n",
    "Calculate loss and optimizing\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "\n",
    "\n",
    "class WaveNet:\n",
    "    def __init__(self, layer_size, stack_size, in_channels, res_channels, lr=0.002):\n",
    "\n",
    "        self.net = WaveNetModule(layer_size, stack_size, in_channels, res_channels)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.receptive_fields = self.net.receptive_fields\n",
    "\n",
    "        self.lr = lr\n",
    "        self.loss = self._loss()\n",
    "        self.optimizer = self._optimizer()\n",
    "\n",
    "        self._prepare_for_gpu()\n",
    "\n",
    "    @staticmethod\n",
    "    def _loss():\n",
    "        loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            loss = loss.cuda()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _optimizer(self):\n",
    "        return torch.optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "\n",
    "    def _prepare_for_gpu(self):\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(\"{0} GPUs are detected.\".format(torch.cuda.device_count()))\n",
    "            self.net = torch.nn.DataParallel(self.net)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.net.cuda()\n",
    "\n",
    "    def train(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Train 1 time\n",
    "        :param inputs: Tensor[batch, timestep, channels]\n",
    "        :param targets: Torch tensor [batch, timestep, channels]\n",
    "        :return: float loss\n",
    "        \"\"\"\n",
    "        outputs = self.net(inputs)\n",
    "\n",
    "        loss = self.loss(outputs.view(-1, self.in_channels),\n",
    "                         targets.long().view(-1))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.data[0]\n",
    "\n",
    "    def generate(self, inputs):\n",
    "        \"\"\"\n",
    "        Generate 1 time\n",
    "        :param inputs: Tensor[batch, timestep, channels]\n",
    "        :return: Tensor[batch, timestep, channels]\n",
    "        \"\"\"\n",
    "        outputs = self.net(inputs)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    @staticmethod\n",
    "    def get_model_path(model_dir, step=0):\n",
    "        basename = 'wavenet'\n",
    "\n",
    "        if step:\n",
    "            return os.path.join(model_dir, '{0}_{1}.pkl'.format(basename, step))\n",
    "        else:\n",
    "            return os.path.join(model_dir, '{0}.pkl'.format(basename))\n",
    "\n",
    "    def load(self, model_dir, step=0):\n",
    "        \"\"\"\n",
    "        Load pre-trained model\n",
    "        :param model_dir:\n",
    "        :param step:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        print(\"Loading model from {0}\".format(model_dir))\n",
    "\n",
    "        model_path = self.get_model_path(model_dir, step)\n",
    "\n",
    "        self.net.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    def save(self, model_dir, step=0):\n",
    "        print(\"Saving model into {0}\".format(model_dir))\n",
    "\n",
    "        model_path = self.get_model_path(model_dir, step)\n",
    "\n",
    "        torch.save(self.net.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Bn4J_fSVkAu"
   },
   "source": [
    "**Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tJp1TRl1Vnu7"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training Options\n",
    "\"\"\"\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--layer_size', type=int, default=10,\n",
    "                    help='layer_size: 10 = layer[dilation=1, dilation=2, 4, 8, 16, 32, 64, 128, 256, 512]')\n",
    "parser.add_argument('--stack_size', type=int, default=5,\n",
    "                    help='stack_size: 5 = stack[layer1, layer2, layer3, layer4, layer5]')\n",
    "parser.add_argument('--in_channels', type=int, default=256,\n",
    "                    help='input channel size. mu-law encode factor, one-hot size')\n",
    "parser.add_argument('--res_channels', type=int, default=512, help='number of channel for residual network')\n",
    "\n",
    "parser.add_argument('--sample_rate', type=int, default=16000, help='Sampling rates for input sound')\n",
    "parser.add_argument('--sample_size', type=int, default=100000, help='Sample size for training input')\n",
    "\n",
    "\n",
    "def parse_args(is_training=True):\n",
    "    if is_training:\n",
    "        parser.add_argument('--data_dir', type=str, default='./test/data', help='Training data dir')\n",
    "        parser.add_argument('--output_dir', type=str, default='./output', help='Output dir for saving model and etc')\n",
    "        parser.add_argument('--num_steps', type=int, default=100000, help='Total training steps')\n",
    "        parser.add_argument('--lr', type=float, default=0.0002, help='learning rate decay')\n",
    "    else:\n",
    "        parser.add_argument('--model_dir', type=str, required=True, help='Pre-trained model dir')\n",
    "        parser.add_argument('--step', type=int, default=0, help='A specific step of pre-trained model to use')\n",
    "        parser.add_argument('--seed', type=str, help='A seed file to generate sound')\n",
    "        parser.add_argument('--out', type=str, help='Output file name which is generated')\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def print_help():\n",
    "    parser.print_help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_CsPOVkrVc8I"
   },
   "source": [
    "**Training Begins**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 619,
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3692,
     "status": "error",
     "timestamp": 1522441243835,
     "user": {
      "displayName": "Narain Adithya",
      "photoUrl": "//lh6.googleusercontent.com/-po2CrL1CsSQ/AAAAAAAAAAI/AAAAAAAAGj0/Oj2RKONZ9Ho/s50-c-k-no/photo.jpg",
      "userId": "102781842695092160061"
     },
     "user_tz": 240
    },
    "id": "b_FsZH1-RsMh",
    "outputId": "bf44bdc6-eb57-47c2-ac5c-c8f0706b63a7"
   },
   "outputs": [
    {
     "ename": "NoBackendError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoBackendError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9fc09878b7dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-9fc09878b7dc>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtotal_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfinite_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwavenet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-9fc09878b7dc>\u001b[0m in \u001b[0;36minfinite_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minfinite_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-wavenet/venv/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-wavenet/venv/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-616d49134923>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mraw_audio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mencoded_audio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmu_law_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_audio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-616d49134923>\u001b[0m in \u001b[0;36mload_audio\u001b[0;34m(filename, sample_rate, trim, trim_frame_length)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_frame_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmono\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-wavenet/venv/lib/python3.5/site-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0maudioread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mn_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-wavenet/venv/lib/python3.5/site-packages/audioread/__init__.py\u001b[0m in \u001b[0;36maudio_open\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;31m# All backends failed!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNoBackendError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNoBackendError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A script for WaveNet training\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.wavenet = WaveNet(10, 5,\n",
    "                               256, 512,\n",
    "                               lr=0.0002)\n",
    "        \n",
    "        self.data_loader = DataLoader(\"./data\", self.wavenet.receptive_fields,\n",
    "                                      100000, 16000, 256)\n",
    "  \n",
    "    def infinite_batch(self):\n",
    "        while True:\n",
    "            for dataset in self.data_loader:\n",
    "                for inputs, targets in dataset:\n",
    "                    yield inputs, targets\n",
    "\n",
    "    def run(self):\n",
    "        total_steps = 0\n",
    "\n",
    "        for inputs, targets in self.infinite_batch():\n",
    "            loss = self.wavenet.train(inputs, targets)\n",
    "\n",
    "            total_steps += 1\n",
    "\n",
    "            print('[{0}/{1}] loss: {2}'.format(total_steps, 20, loss))\n",
    "\n",
    "            if total_steps > 20:\n",
    "                break\n",
    "\n",
    "        self.wavenet.save(\"data\")\n",
    "\n",
    "\n",
    "def prepare_output_dir(args):\n",
    "    args.log_dir = os.path.join(args.output_dir, 'log')\n",
    "    args.model_dir = os.path.join(args.output_dir, 'model')\n",
    "    args.test_output_dir = os.path.join(args.output_dir, 'test')\n",
    "\n",
    "    os.makedirs(args.log_dir, exist_ok=True)\n",
    "    os.makedirs(args.model_dir, exist_ok=True)\n",
    "    os.makedirs(args.test_output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# args = parse_args()\n",
    "\n",
    "# prepare_output_dir(args)\n",
    "\n",
    "trainer = Trainer()\n",
    "\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from data\n",
      "5115/16000 samples are generated.\n",
      "10230/16000 samples are generated.\n",
      "15345/16000 samples are generated.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:58",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3d84eaf1b7b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Generate {0} seconds took {1}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-3d84eaf1b7b0>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_seed_from_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"seed\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwavenet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{0}/{1} samples are generated.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-5e7fb7166587>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \"\"\"\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-wavenet/venv/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-c67682eeadff>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcausal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0mskip_connections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskip_connections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-wavenet/venv/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-c67682eeadff>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, skip_size)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mres_block\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;31m# output is the next input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0mskip_connections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-wavenet/venv/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-c67682eeadff>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, skip_size)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# PixelCNN gate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mgated_tanh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_tanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mgated_sigmoid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_sigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mgated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgated_tanh\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgated_sigmoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-wavenet/venv/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-wavenet/venv/lib/python3.5/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:58"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "class Generator:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.wavenet = WaveNet(args[\"layer_size\"], args[\"stack_size\"],\n",
    "                               args[\"in_channels\"], args[\"res_channels\"])\n",
    "        \n",
    "        self.wavenet.load(args[\"model_dir\"], args[\"step\"])\n",
    "        \n",
    "    @staticmethod\n",
    "    def _variable(data):\n",
    "        tensor = torch.from_numpy(data).float()\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.autograd.Variable(tensor.cuda())\n",
    "        else:\n",
    "            return torch.autograd.Variable(tensor)\n",
    "    def _make_seed(self, audio):\n",
    "        audio = np.pad([audio], [[0, 0], [self.wavenet.receptive_fields, 0], [0, 0]], 'constant')\n",
    "        if \"sample_size\" in self.args and self.args[\"sample_size\"]:\n",
    "            seed = audio[:, :self.args[\"sample_size\"], :]\n",
    "        else:\n",
    "            seed = audio[:, :self.wavenet.receptive_fields*2, :]\n",
    "        return seed\n",
    "    def _get_seed_from_audio(self, filepath):\n",
    "        audio = load_audio(filepath, self.args[\"sample_rate\"])\n",
    "        audio_length = len(audio)\n",
    "        audio = mu_law_encode(audio, self.args[\"in_channels\"])\n",
    "        audio = one_hot_encode(audio, self.args[\"in_channels\"])\n",
    "        seed = self._make_seed(audio)\n",
    "        return self._variable(seed), audio_length\n",
    "    def _save_to_audio_file(self, data):\n",
    "        data = data[0].cpu().data.numpy()\n",
    "        data = one_hot_decode(data, axis=1)\n",
    "        audio = mu_law_decode(data, self.args[\"in_channels\"])\n",
    "        librosa.output.write_wav(self.args[\"out\"], audio, self.args[\"sample_rate\"])\n",
    "        print('Saved wav file at {}'.format(self.args[\"out\"]))\n",
    "        return librosa.get_duration(y=audio, sr=self.args[\"sample_rate\"])\n",
    "    def generate(self):\n",
    "        outputs = []\n",
    "        inputs, audio_length = self._get_seed_from_audio(self.args[\"seed\"])\n",
    "        while True:\n",
    "            new = self.wavenet.generate(inputs)\n",
    "            outputs = torch.cat((outputs, new), dim=1) if len(outputs) else new\n",
    "            print('{0}/{1} samples are generated.'.format(len(outputs[0]), audio_length))\n",
    "            if len(outputs[0]) >= audio_length:\n",
    "                break\n",
    "            inputs = torch.cat((inputs[:, :-len(new[0]), :], new), dim=1)\n",
    "        outputs = outputs[:, :audio_length, :]\n",
    "        return self._save_to_audio_file(outputs)\n",
    "if __name__ == '__main__':\n",
    "    args = {\n",
    "        \"layer_size\": 10,\n",
    "        \"stack_size\": 5,\n",
    "        \"in_channels\": 256,\n",
    "        \"res_channels\": 512,\n",
    "        \"model_dir\": \"data\",\n",
    "        \"step\": None,\n",
    "        \"seed\": \"data/sample.wav\",\n",
    "        \"sample_rate\": 16000\n",
    "    }\n",
    "    generator = Generator(args)\n",
    "    start_time = datetime.datetime.now()\n",
    "    duration = generator.generate()\n",
    "    print('Generate {0} seconds took {1}'.format(duration, datetime.datetime.now() - start_time))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "default_view": {},
   "name": "Wavenet.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
